{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssk3cr0xglvV"
   },
   "source": [
    "In this notebook, quality of combinations with weights obtained from notebooks 2-3 is measured on a test set.\n",
    "\n",
    "In the cell below, you need to specify:\n",
    "- The name of the MTEB task\n",
    "- the name of a BERT-like model in the Transformers library\n",
    "- weights obtained from Optuna in dictionary format. If you have no Optuna weights, just leave the dictionary empty\n",
    "- path to the CSV file with the table of weights obtained by our method\n",
    "- list of layers to be used\n",
    "- number (starting at 0) of the layer that showed best quality on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPsbvYCzhFH0"
   },
   "outputs": [],
   "source": [
    "task_name = \"STSBenchmark\" # your mteb task name\n",
    "model_name = \"bert-base-uncased\" # your model name\n",
    "optuna_weights = {}\n",
    "weights_path = ...\n",
    "layers_to_use = list(range(0, 12)) # layers to use in the combination\n",
    "best_layer = 11 # best layer on train part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyphdhMmiXOf"
   },
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-17T12:20:33.033119Z",
     "iopub.status.busy": "2025-04-17T12:20:33.032275Z",
     "iopub.status.idle": "2025-04-17T12:23:53.955930Z",
     "shell.execute_reply": "2025-04-17T12:23:53.955184Z",
     "shell.execute_reply.started": "2025-04-17T12:20:33.033084Z"
    },
    "id": "6Xp9Ycbl0o5L",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install mteb\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T12:23:53.957443Z",
     "iopub.status.busy": "2025-04-17T12:23:53.957216Z",
     "iopub.status.idle": "2025-04-17T12:23:53.969555Z",
     "shell.execute_reply": "2025-04-17T12:23:53.968473Z",
     "shell.execute_reply.started": "2025-04-17T12:23:53.957424Z"
    },
    "id": "JRuwFh_K0o5T"
   },
   "outputs": [],
   "source": [
    "import mteb\n",
    "test_task = mteb.get_task(task_name, eval_splits=[\"test\"], languages=['eng'])\n",
    "train_task = mteb.get_task(task_name, eval_splits=[\"train\"], languages=['eng'])\n",
    "evaluation = mteb.MTEB(tasks=[train_task])\n",
    "evaluation_final = mteb.MTEB(tasks=[test_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-17T12:23:53.972680Z",
     "iopub.status.idle": "2025-04-17T12:23:53.973015Z",
     "shell.execute_reply": "2025-04-17T12:23:53.972844Z",
     "shell.execute_reply.started": "2025-04-17T12:23:53.972829Z"
    },
    "id": "UIhc4Ze60o5b"
   },
   "outputs": [],
   "source": [
    "from mteb.encoder_interface import PromptType\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class CustomModel:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\", layers_to_use=[-1], layers_weights=[1.0], batch_size=64):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True).to(self.device)\n",
    "        self.layers_to_use = layers_to_use\n",
    "        self.layers_weights = layers_weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_layer_embedding(self, batch: list[str], layers: list[int], weights: list[float]) -> np.ndarray:\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            batch, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        model_output = self.model(**encoded_inputs)\n",
    "        hidden_states = model_output.hidden_states[1:]\n",
    "        layers_output = [hidden_states[i] for i in layers]\n",
    "\n",
    "        pooled_layers = []\n",
    "        for i, layer_output in enumerate(layers_output):\n",
    "            input_mask_expanded = encoded_inputs['attention_mask'].unsqueeze(-1).expand(layer_output.size()).float()\n",
    "            sum_embeddings = torch.sum(layer_output * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            mean_pooled = sum_embeddings / sum_mask\n",
    "            pooled_layers.append(weights[i] * mean_pooled)\n",
    "\n",
    "        return torch.sum(torch.stack(pooled_layers), dim=0)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentences: list[str],\n",
    "        task_name: str,\n",
    "        prompt_type: PromptType | None = None,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Encodes the given sentences using the encoder.\n",
    "\n",
    "        Args:\n",
    "            sentences: The sentences to encode.\n",
    "            task_name: The name of the task.  (Not directly used in this example, but kept for MTEB compatibility)\n",
    "            prompt_type: The prompt type to use. (Not directly used in this example, but kept for MTEB compatibility)\n",
    "            **kwargs: Additional arguments to pass to the encoder.\n",
    "\n",
    "        Returns:\n",
    "            The encoded sentences as a numpy array.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_embeddings = []\n",
    "            for i in range(0, len(sentences), self.batch_size):\n",
    "                batch = sentences[i:i + self.batch_size]\n",
    "                combination = self.get_layer_embedding(batch, self.layers_to_use, self.layers_weights)\n",
    "                combination = combination.cpu().numpy()\n",
    "                all_embeddings.append(combination)\n",
    "        return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-17T12:20:08.887519Z",
     "iopub.status.idle": "2025-04-17T12:20:08.887786Z",
     "shell.execute_reply": "2025-04-17T12:20:08.887660Z",
     "shell.execute_reply.started": "2025-04-17T12:20:08.887646Z"
    },
    "id": "eqqpyWUk0o5d"
   },
   "outputs": [],
   "source": [
    "df_optimal_weights = pd.read_csv(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-17T12:20:08.888560Z",
     "iopub.status.idle": "2025-04-17T12:20:08.888894Z",
     "shell.execute_reply": "2025-04-17T12:20:08.888727Z",
     "shell.execute_reply.started": "2025-04-17T12:20:08.888712Z"
    },
    "id": "-RLCmfuH0o5f"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_string_to_list(string):\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def string_to_float_list(string):\n",
    "  try:\n",
    "    string = string.strip('[]')\n",
    "    numbers = string.split()\n",
    "    float_list = [float(num) for num in numbers]\n",
    "    return float_list\n",
    "  except (ValueError, AttributeError):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WmGfsstW0o5i"
   },
   "outputs": [],
   "source": [
    "def test_weights(model_name, df_optimal_weights, layers, best_layer, optuna_weights):\n",
    "    df_optimal_weights['weights'] = df_optimal_weights['weights'].apply(string_to_float_list)\n",
    "    one_w = [1]\n",
    "    model = CustomModel(model_name=model_name, layers_to_use=[11], layers_weights=one_w, batch_size=256)\n",
    "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/last_layer\")\n",
    "    print(f\"{'last layer'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
    "    print()\n",
    "    one_w = [1]\n",
    "    model = CustomModel(model_name=model_name, layers_to_use=[best_layer], layers_weights=one_w, batch_size=256)\n",
    "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/best_layer\")\n",
    "    print(f\"{'best layer'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
    "    print()\n",
    "    equal_w = np.array([1] * len(layers)) / len(layers)\n",
    "    model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=equal_w, batch_size=256)\n",
    "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/equal\")\n",
    "    print(f\"{'equal weights'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
    "    print()\n",
    "    if len(optuna_weights != 0):\n",
    "        optuna_w = np.array(optuna_weights) / np.sum(np.array(optuna_weights))\n",
    "        model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=optuna_w, batch_size=256)\n",
    "        quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/optuna\")\n",
    "        print(f\"{'optuna weights'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
    "    layers_df = df_optimal_weights[df_optimal_weights.layers == str(layers)]\n",
    "    for i, row in layers_df.iterrows():\n",
    "        print()\n",
    "        weights = row.weights\n",
    "        model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=weights, batch_size=256)\n",
    "        quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/{row.correlation}\")\n",
    "        print(f\"{row.correlation.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQllGdHp7QcG"
   },
   "source": [
    "### Test weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ianhvfVRqFw"
   },
   "outputs": [],
   "source": [
    "df_optimal_weights = pd.read_csv(weights_path)\n",
    "if len(optuna_weights) == 0:\n",
    "    optuna_w = []\n",
    "else:\n",
    "    optuna_w = [optuna_weights['w_1'], optuna_weights['w_2'], optuna_weights['w_3'], optuna_weights['w_4'], optuna_weights['w_5'], optuna_weights['w_6'],\n",
    "                optuna_weights['w_7'], optuna_weights['w_8'], optuna_weights['w_9'], optuna_weights['w_10'], optuna_weights['w_11'], optuna_weights['w_12']]\n",
    "test_weights(model_name, df_optimal_weights, layers_to_use, best_layer, optuna_w)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7166591,
     "sourceId": 11440424,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7172331,
     "sourceId": 11448035,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
