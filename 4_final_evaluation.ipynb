{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11440424,
          "sourceType": "datasetVersion",
          "datasetId": 7166591
        },
        {
          "sourceId": 11448035,
          "sourceType": "datasetVersion",
          "datasetId": 7172331
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В данном ноутбуке производится замер качества на тестовой выборке комбинаций с весами, полученными в ноутбуках 2-3.\n",
        "\n",
        "В ячейке ниже нужно указать:\n",
        "\n",
        "- название задачи mteb\n",
        "- название BERT-like модели в библиотеке transformers\n",
        "- веса, полученные с optuna в формате словаря. Если веса с optuna не подбирались, словарь стоит оставить пустым.\n",
        "- путь к csv файлу с таблицей весов, полученных с помощью нашего метода\n",
        "- список слоев, которые предполагается использовать\n",
        "- номер (нумерация с 0) слоя, показавшего лучшее качество на обучающей выборке"
      ],
      "metadata": {
        "id": "ssk3cr0xglvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_name = \"STSBenchmark\" # your mteb task name\n",
        "model_name = \"bert-base-uncased\" # your model name\n",
        "optuna_weights = {}\n",
        "weights_path = ...\n",
        "layers_to_use = list(range(0, 12)) # layers to use in the combination\n",
        "best_layer = 11 # best layer on train part"
      ],
      "metadata": {
        "id": "dPsbvYCzhFH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "pyphdhMmiXOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install mteb\n",
        "clear_output()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T12:20:33.032275Z",
          "iopub.execute_input": "2025-04-17T12:20:33.033119Z",
          "iopub.status.idle": "2025-04-17T12:23:53.955930Z",
          "shell.execute_reply.started": "2025-04-17T12:20:33.033084Z",
          "shell.execute_reply": "2025-04-17T12:23:53.955184Z"
        },
        "collapsed": true,
        "id": "6Xp9Ycbl0o5L"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import mteb\n",
        "test_task = mteb.get_task(task_name, eval_splits=[\"test\"], languages=['eng'])\n",
        "train_task = mteb.get_task(task_name, eval_splits=[\"train\"], languages=['eng'])\n",
        "evaluation = mteb.MTEB(tasks=[train_task])\n",
        "evaluation_final = mteb.MTEB(tasks=[test_task])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T12:23:53.957216Z",
          "iopub.execute_input": "2025-04-17T12:23:53.957443Z",
          "iopub.status.idle": "2025-04-17T12:23:53.969555Z",
          "shell.execute_reply.started": "2025-04-17T12:23:53.957424Z",
          "shell.execute_reply": "2025-04-17T12:23:53.968473Z"
        },
        "id": "JRuwFh_K0o5T"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "from mteb.encoder_interface import PromptType\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "class CustomModel:\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\", layers_to_use=[-1], layers_weights=[1.0], batch_size=64):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True).to(self.device)\n",
        "        self.layers_to_use = layers_to_use\n",
        "        self.layers_weights = layers_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_layer_embedding(self, batch: list[str], layers: list[int], weights: list[float]) -> np.ndarray:\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            batch, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        model_output = self.model(**encoded_inputs)\n",
        "        hidden_states = model_output.hidden_states[1:]\n",
        "        layers_output = [hidden_states[i] for i in layers]\n",
        "\n",
        "        pooled_layers = []\n",
        "        for i, layer_output in enumerate(layers_output):\n",
        "            input_mask_expanded = encoded_inputs['attention_mask'].unsqueeze(-1).expand(layer_output.size()).float()\n",
        "            sum_embeddings = torch.sum(layer_output * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            mean_pooled = sum_embeddings / sum_mask\n",
        "            pooled_layers.append(weights[i] * mean_pooled)\n",
        "\n",
        "        return torch.sum(torch.stack(pooled_layers), dim=0)\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        sentences: list[str],\n",
        "        task_name: str,\n",
        "        prompt_type: PromptType | None = None,\n",
        "        **kwargs,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Encodes the given sentences using the encoder.\n",
        "\n",
        "        Args:\n",
        "            sentences: The sentences to encode.\n",
        "            task_name: The name of the task.  (Not directly used in this example, but kept for MTEB compatibility)\n",
        "            prompt_type: The prompt type to use. (Not directly used in this example, but kept for MTEB compatibility)\n",
        "            **kwargs: Additional arguments to pass to the encoder.\n",
        "\n",
        "        Returns:\n",
        "            The encoded sentences as a numpy array.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_embeddings = []\n",
        "            for i in range(0, len(sentences), self.batch_size):\n",
        "                batch = sentences[i:i + self.batch_size]\n",
        "                combination = self.get_layer_embedding(batch, self.layers_to_use, self.layers_weights)\n",
        "                combination = combination.cpu().numpy()\n",
        "                all_embeddings.append(combination)\n",
        "        return np.concatenate(all_embeddings, axis=0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T12:23:53.972680Z",
          "iopub.status.idle": "2025-04-17T12:23:53.973015Z",
          "shell.execute_reply.started": "2025-04-17T12:23:53.972829Z",
          "shell.execute_reply": "2025-04-17T12:23:53.972844Z"
        },
        "id": "UIhc4Ze60o5b"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "df_optimal_weights = pd.read_csv(weights_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T12:20:08.887519Z",
          "iopub.status.idle": "2025-04-17T12:20:08.887786Z",
          "shell.execute_reply.started": "2025-04-17T12:20:08.887646Z",
          "shell.execute_reply": "2025-04-17T12:20:08.887660Z"
        },
        "id": "eqqpyWUk0o5d"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def convert_string_to_list(string):\n",
        "    try:\n",
        "        return ast.literal_eval(string)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "def string_to_float_list(string):\n",
        "  try:\n",
        "    string = string.strip('[]')\n",
        "    numbers = string.split()\n",
        "    float_list = [float(num) for num in numbers]\n",
        "    return float_list\n",
        "  except (ValueError, AttributeError):\n",
        "    return None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T12:20:08.888560Z",
          "iopub.status.idle": "2025-04-17T12:20:08.888894Z",
          "shell.execute_reply.started": "2025-04-17T12:20:08.888712Z",
          "shell.execute_reply": "2025-04-17T12:20:08.888727Z"
        },
        "id": "-RLCmfuH0o5f"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "def test_weights(model_name, df_optimal_weights, layers, best_layer, optuna_weights):\n",
        "    df_optimal_weights['weights'] = df_optimal_weights['weights'].apply(string_to_float_list)\n",
        "    one_w = [1]\n",
        "    model = CustomModel(model_name=model_name, layers_to_use=[11], layers_weights=one_w, batch_size=256)\n",
        "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/last_layer\")\n",
        "    print(f\"{'last layer'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
        "    print()\n",
        "    one_w = [1]\n",
        "    model = CustomModel(model_name=model_name, layers_to_use=[best_layer], layers_weights=one_w, batch_size=256)\n",
        "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/best_layer\")\n",
        "    print(f\"{'best layer'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
        "    print()\n",
        "    equal_w = np.array([1] * len(layers)) / len(layers)\n",
        "    model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=equal_w, batch_size=256)\n",
        "    quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/equal\")\n",
        "    print(f\"{'equal weights'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
        "    print()\n",
        "    if len(optuna_weights != 0):\n",
        "        optuna_w = np.array(optuna_weights) / np.sum(np.array(optuna_weights))\n",
        "        model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=optuna_w, batch_size=256)\n",
        "        quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/optuna\")\n",
        "        print(f\"{'optuna weights'.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")\n",
        "    layers_df = df_optimal_weights[df_optimal_weights.layers == str(layers)]\n",
        "    for i, row in layers_df.iterrows():\n",
        "        print()\n",
        "        weights = row.weights\n",
        "        model = CustomModel(model_name=model_name, layers_to_use=layers, layers_weights=weights, batch_size=256)\n",
        "        quality = evaluation_final.run(model, output_folder=f\"results/{model_name}/{layers}/{row.correlation}\")\n",
        "        print(f\"{row.correlation.upper()} accuracy = {quality[0].scores['test'][0]['main_score']}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "WmGfsstW0o5i"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test weights"
      ],
      "metadata": {
        "id": "SQllGdHp7QcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_optimal_weights = pd.read_csv(weights_path)\n",
        "if len(optuna_weights) == 0:\n",
        "    optuna_w = []\n",
        "else:\n",
        "    optuna_w = [optuna_weights['w_1'], optuna_weights['w_2'], optuna_weights['w_3'], optuna_weights['w_4'], optuna_weights['w_5'], optuna_weights['w_6'],\n",
        "                optuna_weights['w_7'], optuna_weights['w_8'], optuna_weights['w_9'], optuna_weights['w_10'], optuna_weights['w_11'], optuna_weights['w_12']]\n",
        "test_weights(model_name, df_optimal_weights, layers_to_use, best_layer, optuna_w)"
      ],
      "metadata": {
        "id": "0ianhvfVRqFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}