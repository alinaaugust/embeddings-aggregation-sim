{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11440424,"sourceType":"datasetVersion","datasetId":7166591},{"sourceId":11542759,"sourceType":"datasetVersion","datasetId":7238705}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Данный ноутбук содержит подбор оптимальных весов в комбинации эмбеддингов с использованием optuna.\n\nВ ячейке ниже нужно указать название задачи mteb, а также название BERT-like модели в библиотеке transformers. По умолчанию оптимизация проводится на 100 итерациях, поэтому может занимать довольно много времени.","metadata":{}},{"cell_type":"code","source":"task_name = \"STSBenchmark\" # your mteb task name\nmodel_name = \"bert-base-uncased\" # your model name\nlayers_to_use = list(range(0, 12)) # layers to use in the combination","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import clear_output\n\n!pip install mteb\n!pip install optuna\nclear_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mteb\ntest_task = mteb.get_task(task_name, eval_splits=[\"test\"], languages=['eng'])\ntrain_task = mteb.get_task(task_name, eval_splits=[\"train\"], languages=['eng'])\nevaluation = mteb.MTEB(tasks=[train_task])\nevaluation_final = mteb.MTEB(tasks=[test_task])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:16:06.490262Z","iopub.execute_input":"2025-05-11T13:16:06.490726Z","iopub.status.idle":"2025-05-11T13:16:07.227925Z","shell.execute_reply.started":"2025-05-11T13:16:06.490684Z","shell.execute_reply":"2025-05-11T13:16:07.227058Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from mteb.encoder_interface import PromptType\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport pandas as pd\nimport optuna\n\nclass CustomModel:\n    def __init__(self, model_name=\"bert-base-uncased\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\", layers_to_use=[-1], layers_weights=[1.0], batch_size=256):\n        self.model_name = model_name\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True).to(self.device)\n        self.layers_to_use = layers_to_use\n        self.layers_weights = layers_weights\n        self.batch_size = batch_size\n\n    def get_layer_embedding(self, batch: list[str], layers: list[int], weights: list[float]) -> np.ndarray:\n        encoded_inputs = self.tokenizer(\n            batch, padding=True, truncation=True, return_tensors=\"pt\"\n        ).to(self.device)\n        model_output = self.model(**encoded_inputs)\n        hidden_states = model_output.hidden_states[1:]\n        layers_output = [hidden_states[i] for i in layers]\n\n        pooled_layers = []\n        for i, layer_output in enumerate(layers_output):\n            input_mask_expanded = encoded_inputs['attention_mask'].unsqueeze(-1).expand(layer_output.size()).float()\n            sum_embeddings = torch.sum(layer_output * input_mask_expanded, 1)\n            sum_mask = input_mask_expanded.sum(1)\n            mean_pooled = sum_embeddings / sum_mask\n            pooled_layers.append(weights[i] * mean_pooled)\n\n        return torch.sum(torch.stack(pooled_layers), dim=0)\n\n    def encode(\n        self,\n        sentences: list[str],\n        task_name: str,\n        prompt_type: PromptType | None = None,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            sentences: The sentences to encode.\n            task_name: The name of the task.  (Not directly used in this example, but kept for MTEB compatibility)\n            prompt_type: The prompt type to use. (Not directly used in this example, but kept for MTEB compatibility)\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded sentences as a numpy array.\n        \"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            all_embeddings = []\n            for i in range(0, len(sentences), self.batch_size):\n                batch = sentences[i:i + self.batch_size]\n                combination = self.get_layer_embedding(batch, self.layers_to_use, self.layers_weights)\n                combination = combination.cpu().numpy()\n                all_embeddings.append(combination)\n        return np.concatenate(all_embeddings, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:16:10.885589Z","iopub.execute_input":"2025-05-11T13:16:10.885884Z","iopub.status.idle":"2025-05-11T13:16:10.895466Z","shell.execute_reply.started":"2025-05-11T13:16:10.885860Z","shell.execute_reply":"2025-05-11T13:16:10.894685Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"counter = 0\nbest_params = {}\ndef run_experiment_all(layers):\n    global counter\n    global model_name\n    layers_to_use = layers\n    def objective(trial):\n        global counter\n        global model_name\n        w_1 = trial.suggest_float('w_1', 0, 1)\n        w_2 = trial.suggest_float('w_2', 0, 1)\n        w_3 = trial.suggest_float('w_3', 0, 1)\n        w_4 = trial.suggest_float('w_4', 0, 1)\n        w_5 = trial.suggest_float('w_5', 0, 1)\n        w_6 = trial.suggest_float('w_6', 0, 1)\n        w_7 = trial.suggest_float('w_7', 0, 1)\n        w_8 = trial.suggest_float('w_8', 0, 1)\n        w_9 = trial.suggest_float('w_9', 0, 1)\n        w_10 = trial.suggest_float('w_10', 0, 1)\n        w_11 = trial.suggest_float('w_11', 0, 1)\n        w_12 = trial.suggest_float('w_12', 0, 1)\n        weights = np.array([w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_10, w_11, w_12])\n        normalized_weights =  weights/np.sum(weights)\n        model = CustomModel(model_name=model_name, layers_to_use=layers_to_use, layers_weights=normalized_weights)\n        quality = evaluation.run(model, output_folder=f\"results/{model_name}/{layers_to_use}/{counter}\")\n        counter += 1\n        return quality[0].scores['train'][0]['main_score']\n        \n    study = optuna.create_study(\n        directions=[\"maximize\"],\n    )\n    study.optimize(objective, n_trials=100)\n    print(\"Number of finished trials: \", len(study.trials))\n    trials = sorted(study.best_trials, key=lambda t: t.values)\n    \n    for trial in trials:\n        print(\"  Trial#{}\".format(trial.number))\n        print(\n            \"    Values: Values={}\".format( \n                trial.values\n            )\n        )\n        print(\"    Params: {}\".format(trial.params))\n        best_params = trial.params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:16:29.925773Z","iopub.execute_input":"2025-05-11T13:16:29.926065Z","iopub.status.idle":"2025-05-11T13:16:29.933901Z","shell.execute_reply.started":"2025-05-11T13:16:29.926041Z","shell.execute_reply":"2025-05-11T13:16:29.932921Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"run_experiment_all(layers_to_use)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}